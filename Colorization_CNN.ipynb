{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nvm45PzvmczH"
      },
      "source": [
        "### Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MPNK98Emmf9j"
      },
      "source": [
        "The aim is to build a machine learning model to automatically turn grayscale images into colored images. The model will be built from scratch (using PyTorch) i.e. without using any pre trained model.\n",
        "\n",
        "At the end, the model will be able to colorize grayscale (or black and white) images. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jqLZaL9s6peb"
      },
      "source": [
        "### Overview \n",
        "\n",
        "In image colorization, the goal is to produce a colored image given a grayscale input image. This problem is challenging because it is multimodal -- a single grayscale image may correspond to many plausible colored images. As a result, traditional models often relied on significant user input alongside a grayscale image. \n",
        "\n",
        "#### The Problem\n",
        "\n",
        "The aim is to infer a full-colored image, which has 3 values per pixel (lightness, saturation, and hue), from a grayscale image, which has only 1 value per pixel (lightness only). \n",
        "To keep things simple for the time being, the model will only work with images of size $256 \\times 256$, so our inputs are of size $256 \\times 256 \\times 1$ (the lightness channel) and our outputs are of size  $256 \\times 256 \\times 2$ (the other two channels). In future, the aim is to improve the framework so that it can work for images of any resolution.\n",
        "\n",
        "\n",
        "Rather than work with images in the RGB format, as people usually do, we will work with them in the [LAB colorspace](https://en.wikipedia.org/wiki/CIELAB_color_space) ($L$ightness, $A$, and $B$) . This colorspace contains exactly the same information as RGB, but it will make it easier for us to separate out the lightness channel from the other two (which we call $A$ and $B$). We'll try to predict the color values of the input image directly (we call this regression)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-8QLJ73wB_C_"
      },
      "source": [
        "#### The Data\n",
        "\n",
        "Getting the data for colorisation problem is not as difficult as other problems as grayscale images can be found anywhere! :p.\n",
        "For this project, we'll use a subset of the [MIT Places](http://places.csail.mit.edu/) dataset of places, landscapes, and buildings. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 230
        },
        "collapsed": true,
        "id": "teReuiWIB_Un",
        "outputId": "e48c7f85-5fb6-4b01-eb8b-b057a581e44d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2018-05-15 13:31:42--  http://data.csail.mit.edu/places/places205/testSetPlaces205_resize.tar.gz\n",
            "Resolving data.csail.mit.edu (data.csail.mit.edu)... 128.52.129.35\n",
            "Connecting to data.csail.mit.edu (data.csail.mit.edu)|128.52.129.35|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2341250899 (2.2G) [application/octet-stream]\n",
            "Saving to: ‘testSetPlaces205_resize.tar.gz’\n",
            "\n",
            " testSetPlaces205_r  12%[=>                  ] 280.81M  5.53MB/s    eta 4m 53s "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              testS  99%[==================> ]   2.18G  5.07MB/s    eta 1s     "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\rtestSetPlaces205_re 100%[===================>]   2.18G  5.06MB/s    in 6m 33s  \r\n",
            "\r\n",
            "2018-05-15 13:38:15 (5.67 MB/s) - ‘testSetPlaces205_resize.tar.gz’ saved [2341250899/2341250899]\r\n",
            "\r\n"
          ]
        }
      ],
      "source": [
        "# Download the data(2.2GB)\n",
        "!wget http://data.csail.mit.edu/places/places205/testSetPlaces205_resize.tar.gz\n",
        "\n",
        "#unzip\n",
        "!tar -xzf testSetPlaces205_resize.tar.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iGNpax-brHmz"
      },
      "outputs": [],
      "source": [
        "# Move data into training and validation directories\n",
        "import os\n",
        "os.makedirs('images/train/class/', exist_ok=True) # 40,000 images\n",
        "os.makedirs('images/val/class/', exist_ok=True)   #  1,000 images\n",
        "for i, file in enumerate(os.listdir('testSet_resize')):\n",
        "  if i < 1000: # first 1000 will be val\n",
        "    os.rename('testSet_resize/' + file, 'images/val/class/' + file)\n",
        "  else: # others will be train\n",
        "    os.rename('testSet_resize/' + file, 'images/train/class/' + file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rRoQxRmuWqnG"
      },
      "outputs": [],
      "source": [
        "# Make sure the images are there\n",
        "from IPython.display import Image, display\n",
        "display(Image(filename='images/val/class/84b3ccd8209a4db1835988d28adfed4c.jpg'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3fb6ZO_mIby3"
      },
      "source": [
        "#### Dependecies\n",
        "\n",
        "We'll build and train our model with PyTorch. We'll also use torchvision, a helpful set of tools for working with images and videos in PyTorch, and scikit-learn for converting between RGB and LAB colorspces. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 589
        },
        "collapsed": true,
        "id": "kDfhTu8oInaO",
        "outputId": "401a0635-8c5b-427d-8a18-b55fb4b0be8c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pillow==4.3.0\n",
            "  Downloading Pillow-4.3.0.tar.gz (13.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 13.9 MB 5.3 MB/s \n",
            "\u001b[?25hCollecting olefile\n",
            "  Downloading olefile-0.46.zip (112 kB)\n",
            "\u001b[K     |████████████████████████████████| 112 kB 63.3 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pillow, olefile\n",
            "  Building wheel for pillow (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pillow: filename=Pillow-4.3.0-cp37-cp37m-linux_x86_64.whl size=1064661 sha256=3145c55261d9c89c1882a5807fc513ddd631a13a135d9c20d4dfefef97f21f50\n",
            "  Stored in directory: /root/.cache/pip/wheels/77/77/72/045aa57329073d3cdbd822995f8228c95d17cce12f1ca75f8f\n",
            "  Building wheel for olefile (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for olefile: filename=olefile-0.46-py2.py3-none-any.whl size=35432 sha256=db8cdd16378ce1688ef90748f337b1c8a32ec97a45ff0b141b84b097ad70e2ac\n",
            "  Stored in directory: /root/.cache/pip/wheels/84/53/e6/37d90ccb3ad1a3ca98d2b17107e9fda401a7c541ea1eb6a65a\n",
            "Successfully built pillow olefile\n",
            "Installing collected packages: olefile, pillow\n",
            "  Attempting uninstall: pillow\n",
            "    Found existing installation: Pillow 7.1.2\n",
            "    Uninstalling Pillow-7.1.2:\n",
            "      Successfully uninstalled Pillow-7.1.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.12.0+cu113 requires pillow!=8.3.*,>=5.3.0, but you have pillow 4.3.0 which is incompatible.\n",
            "fastai 2.6.3 requires pillow>6.0.0, but you have pillow 4.3.0 which is incompatible.\n",
            "bokeh 2.3.3 requires pillow>=7.1.0, but you have pillow 4.3.0 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed olefile-0.46 pillow-4.3.0\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Download and import libraries\n",
        "!pip install pillow==4.3.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J_Y9UFYhKbuE"
      },
      "outputs": [],
      "source": [
        "# For plotting\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "# For conversion\n",
        "from skimage.color import lab2rgb, rgb2lab, rgb2gray\n",
        "from skimage import io\n",
        "# For everything\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "# For our model\n",
        "import torchvision.models as models\n",
        "from torchvision import datasets, transforms\n",
        "# For utilities\n",
        "import os, shutil, time\n",
        "from PIL import Image, ImageOps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gMolrzKA_1uJ"
      },
      "outputs": [],
      "source": [
        "# Check if GPU is available\n",
        "use_gpu = torch.cuda.is_available()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UuXUTKG16pbT"
      },
      "source": [
        "\n",
        "### The Model\n",
        "\n",
        "Our model is a convolutional neural network. We first apply a number of convolutional layers to extract features from our image, and then we apply deconvolutional layers to upscale (increase the spacial resolution) of our features.  \n",
        "\n",
        "Specifically, the beginning of our model will be ResNet-18, an image classification network with 18 layers and residual connections. We will modify the first layer of the network so that it accepts grayscale input rather than colored input. Only first 6 layers of the ResNet-18 will be used for our purpose.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FKxYZNJjJym1"
      },
      "source": [
        "Now, we'll define our model in code. We'll start with the second half of the net, the upsampling layers:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NOQfP1feNkfS"
      },
      "outputs": [],
      "source": [
        "class ColorizationNet(nn.Module):\n",
        "  def __init__(self, input_size=128):\n",
        "    super(ColorizationNet, self).__init__()\n",
        "    MIDLEVEL_FEATURE_SIZE = 128\n",
        "\n",
        "    ## First half: ResNet\n",
        "    resnet = models.resnet18(num_classes=365) \n",
        "    # Change first conv layer to accept single-channel (grayscale) input\n",
        "    resnet.conv1.weight = nn.Parameter(resnet.conv1.weight.sum(dim=1).unsqueeze(1)) \n",
        "    # Extract midlevel features from ResNet-gray\n",
        "    self.midlevel_resnet = nn.Sequential(*list(resnet.children())[0:6])\n",
        "\n",
        "    ## Second half: Upsampling\n",
        "    self.upsample = nn.Sequential(     \n",
        "      nn.Conv2d(MIDLEVEL_FEATURE_SIZE, 128, kernel_size=3, stride=1, padding=1),\n",
        "      nn.BatchNorm2d(128),\n",
        "      nn.ReLU(),\n",
        "      nn.Upsample(scale_factor=2),\n",
        "      nn.Conv2d(128, 64, kernel_size=3, stride=1, padding=1),\n",
        "      nn.BatchNorm2d(64),\n",
        "      nn.ReLU(),\n",
        "      nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),\n",
        "      nn.BatchNorm2d(64),\n",
        "      nn.ReLU(),\n",
        "      nn.Upsample(scale_factor=2),\n",
        "      nn.Conv2d(64, 32, kernel_size=3, stride=1, padding=1),\n",
        "      nn.BatchNorm2d(32),\n",
        "      nn.ReLU(),\n",
        "      nn.Conv2d(32, 2, kernel_size=3, stride=1, padding=1),\n",
        "      nn.Upsample(scale_factor=2)\n",
        "    )\n",
        "\n",
        "  def forward(self, input):\n",
        "\n",
        "    # Pass input through ResNet-gray to extract features\n",
        "    midlevel_features = self.midlevel_resnet(input)\n",
        "\n",
        "    # Upsample to get colors\n",
        "    output = self.upsample(midlevel_features)\n",
        "    return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AREpHNkJTfQr"
      },
      "source": [
        "Now let's create our model and load it onto the GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OtRkAkIjTeq1"
      },
      "outputs": [],
      "source": [
        "model = ColorizationNet()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4444sq_mUbam"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FVlUZTb2Jyjg"
      },
      "source": [
        "#### Loss Function\n",
        "\n",
        "Since we are doing regression, we'll use a mean squared error loss function: we minimize the squared distance between the color value we try to predict, and the true (ground-truth) color value.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1PU0wtkPRasL"
      },
      "outputs": [],
      "source": [
        "criterion = nn.MSELoss()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XUbzxonkRBwE"
      },
      "source": [
        "This loss function is slightly problematic for colorization due to the multi-modality of the problem. \n",
        "\n",
        "For example, if a gray dress could be red or blue, and our model picks the wrong color, it will be harshly penalized. As a result, our model will usually choose desaturated colors that are less likely to be \"very wrong\" than bright, vibrant colors. There has been [significant research](http://richzhang.github.io/colorization/) on this issue, but we will stick to our simple loss function for today.  \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ouggKQZhTT7d"
      },
      "source": [
        "#### Optimizer\n",
        "\n",
        "We will optimize our loss function (criterion) with the Adam optimizer. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j3YZ3977TTl4"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2, weight_decay=0.0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "weOUBFvIUh-C"
      },
      "source": [
        "#### Loading the data\n",
        "\n",
        "We'll use torchtext to load the data. Since we want images in the LAB space, we first have to define a custom dataloader to convert the images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "05NviQfzU2Mv"
      },
      "outputs": [],
      "source": [
        "class GrayscaleImageFolder(datasets.ImageFolder):\n",
        "  '''Custom images folder, which converts images to grayscale before loading'''\n",
        "  def __getitem__(self, index):\n",
        "    path, target = self.imgs[index]\n",
        "    img = self.loader(path)\n",
        "    if self.transform is not None:\n",
        "      img_original = self.transform(img)\n",
        "      img_original = np.asarray(img_original)\n",
        "      img_lab = rgb2lab(img_original)\n",
        "      img_lab = (img_lab + 128) / 255\n",
        "      img_ab = img_lab[:, :, 1:3]\n",
        "      img_ab = torch.from_numpy(img_ab.transpose((2, 0, 1))).float()\n",
        "      img_original = rgb2gray(img_original)\n",
        "      img_original = torch.from_numpy(img_original).unsqueeze(0).float()\n",
        "    if self.target_transform is not None:\n",
        "      target = self.target_transform(target)\n",
        "    return img_original, img_ab, target"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ma3PXDqRBsS"
      },
      "source": [
        "Next we define transforms for our training and validation data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dmG6NAhdVNWw"
      },
      "outputs": [],
      "source": [
        "# Training\n",
        "train_transforms = transforms.Compose([transforms.RandomResizedCrop(224), transforms.RandomHorizontalFlip()])\n",
        "train_imagefolder = GrayscaleImageFolder('images/train', train_transforms)\n",
        "train_loader = torch.utils.data.DataLoader(train_imagefolder, batch_size=64, shuffle=True)\n",
        "\n",
        "# Validation \n",
        "val_transforms = transforms.Compose([transforms.Resize(256), transforms.CenterCrop(224)])\n",
        "val_imagefolder = GrayscaleImageFolder('images/val' , val_transforms)\n",
        "val_loader = torch.utils.data.DataLoader(val_imagefolder, batch_size=64, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t11qIZetsJL9"
      },
      "source": [
        "#### Helper functions\n",
        "\n",
        "Before we train, we define helper functions for tracking the training loss and converting images back to RGB. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2FDRt12jsIkr"
      },
      "outputs": [],
      "source": [
        "class AverageMeter(object):\n",
        "  '''A handy class from the PyTorch ImageNet tutorial''' \n",
        "  def __init__(self):\n",
        "    self.reset()\n",
        "  def reset(self):\n",
        "    self.val, self.avg, self.sum, self.count = 0, 0, 0, 0\n",
        "  def update(self, val, n=1):\n",
        "    self.val = val\n",
        "    self.sum += val * n\n",
        "    self.count += n\n",
        "    self.avg = self.sum / self.count\n",
        "\n",
        "def to_rgb(grayscale_input, ab_input, save_path=None, save_name=None):\n",
        "  '''Show/save rgb image from grayscale and ab channels\n",
        "     Input save_path in the form {'grayscale': '/path/', 'colorized': '/path/'}'''\n",
        "  plt.clf() # clear matplotlib \n",
        "  color_image = torch.cat((grayscale_input, ab_input), 0).numpy() # combine channels\n",
        "  color_image = color_image.transpose((1, 2, 0))  # rescale for matplotlib\n",
        "  color_image[:, :, 0:1] = color_image[:, :, 0:1] * 100\n",
        "  color_image[:, :, 1:3] = color_image[:, :, 1:3] * 255 - 128   \n",
        "  color_image = lab2rgb(color_image.astype(np.float64))\n",
        "  grayscale_input = grayscale_input.squeeze().numpy()\n",
        "  if save_path is not None and save_name is not None: \n",
        "    plt.imsave(arr=grayscale_input, fname='{}{}'.format(save_path['grayscale'], save_name), cmap='gray')\n",
        "    plt.imsave(arr=color_image, fname='{}{}'.format(save_path['colorized'], save_name))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YNoVCs_DriXu"
      },
      "source": [
        "#### Validation\n",
        "\n",
        "In validation, we simply run model without backpropagation using `torch.no_grad`. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M_OTbyOcrh9J"
      },
      "outputs": [],
      "source": [
        "def validate(val_loader, model, criterion, save_images, epoch):\n",
        "  model.eval()\n",
        "\n",
        "  # Prepare value counters and timers\n",
        "  batch_time, data_time, losses = AverageMeter(), AverageMeter(), AverageMeter()\n",
        "\n",
        "  end = time.time()\n",
        "  already_saved_images = False\n",
        "  for i, (input_gray, input_ab, target) in enumerate(val_loader):\n",
        "    data_time.update(time.time() - end)\n",
        "\n",
        "    # Use GPU\n",
        "    if use_gpu: input_gray, input_ab, target = input_gray.cuda(), input_ab.cuda(), target.cuda()\n",
        "\n",
        "    # Run model and record loss\n",
        "    output_ab = model(input_gray) # throw away class predictions\n",
        "    loss = criterion(output_ab, input_ab)\n",
        "    losses.update(loss.item(), input_gray.size(0))\n",
        "\n",
        "    # Save images to file\n",
        "    if save_images and not already_saved_images:\n",
        "      already_saved_images = True\n",
        "      for j in range(min(len(output_ab), 10)): # save at most 5 images\n",
        "        save_path = {'grayscale': 'outputs/gray/', 'colorized': 'outputs/color/'}\n",
        "        save_name = 'img-{}-epoch-{}.jpg'.format(i * val_loader.batch_size + j, epoch)\n",
        "        to_rgb(input_gray[j].cpu(), ab_input=output_ab[j].detach().cpu(), save_path=save_path, save_name=save_name)\n",
        "\n",
        "    # Record time to do forward passes and save images\n",
        "    batch_time.update(time.time() - end)\n",
        "    end = time.time()\n",
        "\n",
        "    # Print model accuracy -- in the code below, val refers to both value and validation\n",
        "    if i % 25 == 0:\n",
        "      print('Validate: [{0}/{1}]\\t'\n",
        "            'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
        "            'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'.format(\n",
        "             i, len(val_loader), batch_time=batch_time, loss=losses))\n",
        "\n",
        "  print('Finished validation.')\n",
        "  return losses.avg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PkSnVRWovBot"
      },
      "source": [
        "#### Training\n",
        "\n",
        "In training, run model and backpropagate using `loss.backward()`. We first define a function that trains for one epoch: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FOo__iEnvLMB"
      },
      "outputs": [],
      "source": [
        "def train(train_loader, model, criterion, optimizer, epoch):\n",
        "  print('Starting training epoch {}'.format(epoch))\n",
        "  model.train()\n",
        "  \n",
        "  # Prepare value counters and timers\n",
        "  batch_time, data_time, losses = AverageMeter(), AverageMeter(), AverageMeter()\n",
        "\n",
        "  end = time.time()\n",
        "  for i, (input_gray, input_ab, target) in enumerate(train_loader):\n",
        "    \n",
        "    # Use GPU if available\n",
        "    if use_gpu: input_gray, input_ab, target = input_gray.cuda(), input_ab.cuda(), target.cuda()\n",
        "\n",
        "    # Record time to load data (above)\n",
        "    data_time.update(time.time() - end)\n",
        "\n",
        "    # Run forward pass\n",
        "    output_ab = model(input_gray) \n",
        "    loss = criterion(output_ab, input_ab) \n",
        "    losses.update(loss.item(), input_gray.size(0))\n",
        "\n",
        "    # Compute gradient and optimize\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Record time to do forward and backward passes\n",
        "    batch_time.update(time.time() - end)\n",
        "    end = time.time()\n",
        "\n",
        "    # Print model accuracy -- in the code below, val refers to value, not validation\n",
        "    if i % 25 == 0:\n",
        "      print('Epoch: [{0}][{1}/{2}]\\t'\n",
        "            'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
        "            'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
        "            'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'.format(\n",
        "              epoch, i, len(train_loader), batch_time=batch_time,\n",
        "             data_time=data_time, loss=losses)) \n",
        "\n",
        "  print('Finished training epoch {}'.format(epoch))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ge7ffXSSRBjj"
      },
      "source": [
        "Next, we define a training loop and we train for 100 epochs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eeHK3BUtANrw"
      },
      "outputs": [],
      "source": [
        "# Move model and loss function to GPU\n",
        "if use_gpu: \n",
        "  criterion = criterion.cuda()\n",
        "  model = model.cuda()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ckNmKA5VwSh1"
      },
      "outputs": [],
      "source": [
        "# Make folders and set parameters\n",
        "os.makedirs('outputs/color', exist_ok=True)\n",
        "os.makedirs('outputs/gray', exist_ok=True)\n",
        "os.makedirs('checkpoints', exist_ok=True)\n",
        "save_images = True\n",
        "best_losses = 1e10\n",
        "epochs = 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 625
        },
        "collapsed": true,
        "id": "XUR6ALi3AZoO",
        "outputId": "4b274c7b-fc1d-4e14-dc1f-a7ef93f3663f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting training epoch 0\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-4a2b342b6ec7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0;31m# Train for one epoch, then validate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m   \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-33-f85e5582e434>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(train_loader, model, criterion, optimizer, epoch)\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;31m# Compute gradient and optimize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     87\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     88\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Train model\n",
        "for epoch in range(epochs):\n",
        "  # Train for one epoch, then validate\n",
        "  train(train_loader, model, criterion, optimizer, epoch)\n",
        "  with torch.no_grad():\n",
        "    losses = validate(val_loader, model, criterion, save_images, epoch)\n",
        "  # Save checkpoint and replace old best model if current model is better\n",
        "  if losses < best_losses:\n",
        "    best_losses = losses\n",
        "    torch.save(model.state_dict(), 'checkpoints/model-epoch-{}-losses-{:.3f}.pth'.format(epoch+1,losses))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hERl_wcm7JsQ"
      },
      "source": [
        "### Results "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OnrbjYF39wRO"
      },
      "outputs": [],
      "source": [
        "# Show images \n",
        "import matplotlib.image as mpimg\n",
        "image_pairs = [('outputs/color/img-2-epoch-0.jpg', 'outputs/gray/img-2-epoch-0.jpg'),\n",
        "               ('outputs/color/img-7-epoch-0.jpg', 'outputs/gray/img-7-epoch-0.jpg')]\n",
        "for c, g in image_pairs:\n",
        "  color = mpimg.imread(c)\n",
        "  gray  = mpimg.imread(g)\n",
        "  f, axarr = plt.subplots(1, 2)\n",
        "  f.set_size_inches(15, 15)\n",
        "  axarr[0].imshow(gray, cmap='gray')\n",
        "  axarr[1].imshow(color)\n",
        "  axarr[0].axis('off'), axarr[1].axis('off')\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DbmrLau3_uLz"
      },
      "source": [
        "# Testing on a single image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rOT4pbLLUFlx"
      },
      "outputs": [],
      "source": [
        "val_transforms1 = transforms.Compose([transforms.Resize(256), transforms.CenterCrop(224)])\n",
        "iimage = Image.open('/content/wom.jpg')\n",
        "iiorig = val_transforms1(iimage)\n",
        "iiorig = np.asarray(iiorig)\n",
        "img_lab = rgb2lab(iiorig)\n",
        "img_lab = (img_lab + 128) / 255\n",
        "img_ab = img_lab[:, :, 1:3]\n",
        "img_ab = torch.from_numpy(img_ab.transpose((2, 0, 1))).float()\n",
        "iiorig = rgb2gray(iiorig)\n",
        "iiorig = torch.from_numpy(iiorig).unsqueeze(0).float()\n",
        "iiorig = iiorig.reshape((1,1,224,224))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4WEoohJsZ6pD"
      },
      "outputs": [],
      "source": [
        "output_ab = model(iiorig) \n",
        "save_path = {'grayscale': 'outputs/gray/', 'colorized': 'outputs/color/'}\n",
        "save_name = 'wom.jpg'\n",
        "to_rgb(iiorig[0].cpu(), ab_input=output_ab[0].detach().cpu(), save_path=save_path, save_name=save_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DmZ1tVU2AToF"
      },
      "source": [
        "# Working with images of any dimensions (WORK IN PROGRESS!)\n",
        "\n",
        "Idea:\n",
        "1. Use sliding window (window size 256 x 256) on the input image. \n",
        "2. Make predictions on individual patches. \n",
        "3. Take the average of the overlapping area.\n",
        "4. Recombine the patches to form a complete image.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_rdUkqgntJ3g",
        "outputId": "afdcd69c-7f3c-4b88-d1a9-4ae8b160eeb7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:10: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ../torch/csrc/utils/tensor_numpy.cpp:178.)\n",
            "  # Remove the CWD from sys.path while we load stuff.\n"
          ]
        }
      ],
      "source": [
        "val_transforms1 = transforms.Compose([transforms.Resize(256), transforms.CenterCrop(224)])\n",
        "iimage = Image.fromarray(patch_rnd)\n",
        "iiorig = val_transforms1(iimage)\n",
        "iiorig = np.asarray(iiorig)\n",
        "# img_lab = rgb2lab(iiorig)\n",
        "# img_lab = (img_lab + 128) / 255\n",
        "# img_ab = img_lab[:, :, 1:3]\n",
        "# img_ab = torch.from_numpy(img_ab.transpose((2, 0, 1))).float()\n",
        "# iiorig = rgb2gray(iiorig)\n",
        "iiorig = torch.from_numpy(iiorig).unsqueeze(0).float()\n",
        "iiorig = iiorig.reshape((1,1,224,224))\n",
        "output_ab = model(iiorig) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cmip2reHL6ES"
      },
      "outputs": [],
      "source": [
        "def sliding_window(img, patch_size=224,istep=16, jstep=16):\n",
        "  Ni = patch_size\n",
        "  Nj = patch_size\n",
        "  for i in range(0, img.shape[0] - Ni, istep):\n",
        "    for j in range(0, img.shape[1] - Ni, jstep):\n",
        "      patch = img[i:i + Ni, j:j + Nj]\n",
        "      yield (i, j), patch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VsZtWW1c1svT"
      },
      "outputs": [],
      "source": [
        "def prep_img_pred(img_path):\n",
        "  input_image = Image.open(img_path)\n",
        "  input_gray = ImageOps.grayscale(input_image)\n",
        "  test_image = np.asarray(input_gray)\n",
        "  indices, patches = zip(*sliding_window(test_image))\n",
        "  indices = np.asarray(indices)\n",
        "  patches = np.asarray(patches)\n",
        "  patches = patches.reshape((patches.shape[0], 1, patches.shape[1], patches.shape[2]))\n",
        "  patches = torch.from_numpy(patches).float()\n",
        "  return indices, patches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2E4UkfgpMDyp"
      },
      "outputs": [],
      "source": [
        "test_indices, test_patches = prep_img_pred('/content/test.jpg')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "265yVgd6ze1l"
      },
      "outputs": [],
      "source": [
        "test_patches.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MwIM61EKGcI3"
      },
      "outputs": [],
      "source": [
        "iterations = test_patches.shape[0]//8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hsy2PWwFGiz2"
      },
      "outputs": [],
      "source": [
        "iterations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q692BsWnPlVi"
      },
      "outputs": [],
      "source": [
        "outputAB_from_model = model(test_patches[:8,:,:,:])\n",
        "for i in range (1,iterations+1):\n",
        "  st = i*8\n",
        "  en = min(st+8,test_patches.shape[0])\n",
        "  #print(st,en)\n",
        "  if(st<test_patches.shape[0]):\n",
        "    outputAB_from_model = torch.cat((outputAB_from_model,model(test_patches[st:en,:,:,:])),0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GE4IhZyDFiS-"
      },
      "outputs": [],
      "source": [
        "outputAB_from_model.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AHC4DTYz8vi_"
      },
      "outputs": [],
      "source": [
        "input_image = Image.open('/content/test.jpg')\n",
        "input_gray = ImageOps.grayscale(input_image)\n",
        "test_image = np.asarray(input_gray)\n",
        "print(test_image.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xEwXcZfo99oZ"
      },
      "outputs": [],
      "source": [
        "image_recons_skel = np.zeros((3,test_image.shape[0],test_image.shape[1]), dtype=np.float64)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oONWY50D-pyu"
      },
      "outputs": [],
      "source": [
        "image_recons_skel.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HIqHlNMMzrQv"
      },
      "outputs": [],
      "source": [
        "color_image = torch.cat((grayscale_input, ab_input), 0).numpy() # combine channels\n",
        "  color_image = color_image.transpose((1, 2, 0))  # rescale for matplotlib\n",
        "  color_image[:, :, 0:1] = color_image[:, :, 0:1] * 100\n",
        "  color_image[:, :, 1:3] = color_image[:, :, 1:3] * 255 - 128   \n",
        "  color_image = lab2rgb(color_image.astype(np.float64))\n",
        "  grayscale_input = grayscale_input.squeeze().numpy()\n",
        "  if save_path is not None and save_name is not None: \n",
        "    plt.imsave(arr=grayscale_input, fname='{}{}'.format(save_path['grayscale'], save_name), cmap='gray')\n",
        "    plt.imsave(arr=color_image, fname='{}{}'.format(save_path['colorized'], save_name))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I3zmUlGaz7NX"
      },
      "outputs": [],
      "source": [
        "test_patches[342:343,:,:,:].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x-P9M_vO0Dq_"
      },
      "outputs": [],
      "source": [
        "color_image = torch.cat((test_patches[342:343,:,:,:][0], outputAB_from_model[0].detach()), 0).numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "psXinfV81DJB"
      },
      "outputs": [],
      "source": [
        "color_image.shape"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Colorization successful.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.9.7 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.7"
    },
    "vscode": {
      "interpreter": {
        "hash": "f22a20af907fde35ff19e1e892fdb271353fb19b11c7ebd774491472e685293c"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
